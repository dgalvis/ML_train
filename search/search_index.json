{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Machine Learning Tutorials","text":""},{"location":"index.html#introduction","title":"\ud83d\udcd8 Introduction","text":"<p>Welcome! This repository is a personal project designed to demonstrate machine learning techniques and explore tools from the data science and software development ecosystem.</p> <p>The code and notebooks are structured as hands-on tutorials, so feel free to follow along, experiment, and build on them.</p> <p>To get started, please set up your virtual environment. Instructions can be found on the Requirements page.</p>"},{"location":"index.html#supervised-learning","title":"\ud83e\udd16 Supervised Learning","text":"<p>Supervised learning is a machine learning approach where a model is trained on labeled data\u2014that is, input-output pairs. The goal is to learn a general rule that maps inputs to outputs. This type of learning is a form of predictive modelling, where models are trained on historical data to make predictions about new, unseen data.</p>"},{"location":"index.html#examples-of-supervised-learning","title":"\ud83d\udd0d Examples of Supervised Learning","text":"<ul> <li> <p>Forecasting energy consumption   A model is trained on a dataset containing daily energy usage over the course of a year. It then predicts energy usage for future days based on patterns it has learned.   \u2192 This is an example of regression, because the target output (energy usage) is a continuous variable.</p> </li> <li> <p>Email spam detection   A model is trained on a labeled dataset of emails, where each email is marked as spam or not spam. The model learns to classify new, unseen emails accordingly.   \u2192 This is an example of classification, because the target output is a categorical variable (e.g., {\"spam\", \"not spam\"} or {0, 1}).</p> </li> </ul>"},{"location":"index.html#regression","title":"\ud83d\udcc8 Regression","text":"<p>Regression analysis refers to a family of methods used to estimate the relationship between a continuous dependent variable (also called the output, response, or label) and one or more independent variables (also called features, predictors, or regressors).</p>"},{"location":"index.html#regression-examples","title":"\ud83d\udd0d Regression Examples","text":"<ul> <li>Linear Regression</li> <li>(More examples coming soon)</li> </ul>"},{"location":"index.html#classification","title":"\ud83e\uddee Classification","text":"<p>Classification is the task of predicting categorical labels from input data. The model learns to assign new data points to one of a finite set of classes.</p>"},{"location":"index.html#classification-examples","title":"\ud83d\udd0d Classification Examples","text":"<ul> <li>Logistic Regression</li> <li>(More examples coming soon)</li> </ul>"},{"location":"linear_regression.html","title":"Linear Regression","text":""},{"location":"linear_regression.html#introduction","title":"Introduction","text":"<p>Linear regression is a type of supervised learning, where the goal is to identify a best-fit function between a dependent variable and one or more independent variables.</p>"},{"location":"linear_regression.html#model-definition","title":"Model Definition","text":"<p>We assume the model takes the form:</p> \\[ y_i = \\beta_0 + \\beta_1 x_{i,1} + \\dots + \\beta_N x_{i,N} + \\epsilon_i, \\] <p>where \\( i = 1, \\dots, M \\) indexes the samples in the training set.</p> <p>The components of the model are:</p> <ul> <li>\\( x_{i,j} \\) \u2014 Feature \\( j \\) for sample \\( i \\); there are \\( N \\) features.</li> <li>\\( y_i \\) \u2014 The output (target) for sample \\( i \\).</li> <li>\\( \\epsilon_i \\) \u2014 The error term, representing noise or unexplained variation.</li> <li>\\( \\beta_0 \\) \u2014 The intercept (bias).</li> <li>\\( \\beta_j \\) \u2014 The coefficient for feature \\( j \\).</li> </ul>"},{"location":"linear_regression.html#fitting-the-model-ordinary-least-squares-ols","title":"Fitting the Model: Ordinary Least Squares (OLS)","text":"<p>The most common way to fit a linear regression model is with the ordinary least squares (OLS) method. The objective is to find parameters \\( \\boldsymbol{\\beta} \\) that minimise the following cost function:</p> \\[ J(\\boldsymbol{\\beta}) = \\frac{1}{2} \\sum_{i=1}^{M} \\left(h_{\\boldsymbol{\\beta}}(X_i) - y_i\\right)^2, \\] <p>where:</p> <ul> <li>\\( \\boldsymbol{\\beta} = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_N \\end{bmatrix} \\) \u2014 Vector of model parameters.</li> <li>\\( X_i = \\begin{bmatrix} 1 &amp; x_{i,1} &amp; x_{i,2} &amp; \\dots &amp; x_{i,N} \\end{bmatrix} \\) \u2014 Row vector of features for sample \\( i \\), including a 1 to account for the bias term \\( \\beta_0 \\).</li> <li>\\( h_{\\boldsymbol{\\beta}}(X_i) \\) \u2014 The hypothesis (model) function, defined as:</li> </ul> \\[ h_{\\boldsymbol{\\beta}}(X_i) = \\beta_0 + \\beta_1 x_{i,1} + \\dots + \\beta_N x_{i,N} = \\boldsymbol{\\beta}^\\top X_i^\\top. \\] <p>Thus, we are minimising the sum of squared differences between the predicted outputs \\( h_{\\boldsymbol{\\beta}}(X_i) \\) and the actual outputs \\( y_i \\) across all training samples.</p>"},{"location":"linear_regression.html#closed-form-solution","title":"Closed-Form Solution","text":"<p>To simplify notation and enable vectorised computation, we define:</p> <ul> <li>The design matrix:</li> </ul> \\[ X = \\begin{bmatrix} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_M \\end{bmatrix} \\quad \\text{(an \\( M \\times (N+1) \\) matrix)} \\] <ul> <li>The output vector:</li> </ul> \\[ \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_M \\end{bmatrix} \\] <p>OLS is one of the few machine learning methods with a closed-form analytic solution for the best-fit parameters. The solution is:</p> \\[ \\boldsymbol{\\beta} = (X^\\top X)^{-1} X^\\top \\mathbf{y} \\] <p>This gives the value of \\( \\boldsymbol{\\beta} \\) that minimises the cost function and fits the training data as closely as possible in the least-squares sense.</p>"},{"location":"linear_regression.html#example-from-r","title":"Example from <code>R</code>","text":"<p>The programming language <code>R</code> is widely used in statistics and data visualisation. It also provides a useful library of built-in datasets.</p> <p>To explore this, navigate to the folder <code>./codes/</code> in the repository. Activate the virtual environment we created earlier (see Requirements).</p> <p>Then, launch Jupyter Notebook:</p> console<pre><code>jupyter notebook\n</code></pre> <p>Open the notebook named <code>linear_regression_R.ipynb</code>, which contains <code>R</code> code. Make sure you use the <code>R</code> kernel. </p> <p>We begin by importing <code>datasets</code> package:</p> R<pre><code>library(datasets)\n</code></pre> <p>This package includes several classic datasets, one of which is <code>USJudgeRatings</code>. This dataset contains average ratings for 43 judges across several dimensions (e.g., integrity, diligence, writing quality). The 12th column is the rating for \u201cworthy of retention,\u201d which we\u2019ll try to predict using the first 11 features.</p> <p>This command prints the first six rows of the dataset:</p> R<pre><code>dataset = USJudgeRatings\nhead(dataset)\n</code></pre> US Judge Ratings CONTINTGDMNRDILGCFMGDECIPREPFAMIORALWRITPHYSRTEN AARONSON,L.H.5.77.97.77.37.17.47.17.17.17.08.37.8 ALEXANDER,J.M.6.88.98.88.57.88.18.08.07.87.98.58.7 ARMENTANO,A.J.7.28.17.87.87.57.67.57.57.37.47.97.8 BERDON,R.I.6.88.88.58.88.38.58.78.78.48.58.88.7 BRACKEN,J.J.7.36.44.36.56.06.25.75.75.15.35.54.8 BURNS,E.B.6.28.88.78.57.98.08.18.08.08.08.68.6 <p>To prepare the data and run linear regression:</p> R<pre><code>x &lt;- as.matrix(dataset[, c(1:11)]) # First 11 dimensions are Features\ny &lt;- as.matrix(dataset[, 12]) # last dimeions is the Output (Retention rating)\nreg &lt;- lm(y ~ x)\n</code></pre> <p>This creates matrices <code>x</code> and <code>y</code> and uses the <code>lm</code> function to perform linear regression.</p> <p>To see the model summary:</p> R<pre><code>summary(reg)\n</code></pre> <p>This produces several useful statistics: Output<pre><code>Call:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max\n-0.22123 -0.06155 -0.01055  0.05045  0.26079\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -2.11943    0.51904  -4.083 0.000290 ***\nxCONT        0.01280    0.02586   0.495 0.624272\nxINTG        0.36484    0.12936   2.820 0.008291 **\nxDMNR        0.12540    0.08971   1.398 0.172102\nxDILG        0.06669    0.14303   0.466 0.644293\nxCFMG       -0.19453    0.14779  -1.316 0.197735\nxDECI        0.27829    0.13826   2.013 0.052883 .\nxPREP       -0.00196    0.24001  -0.008 0.993536\nxFAMI       -0.13579    0.26725  -0.508 0.614972\nxORAL        0.54782    0.27725   1.976 0.057121 .\nxWRIT       -0.06806    0.31485  -0.216 0.830269\nxPHYS        0.26881    0.06213   4.326 0.000146 ***\n---\nSignif. codes:  0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1\n\nResidual standard error: 0.1174 on 31 degrees of freedom\nMultiple R-squared:  0.9916,    Adjusted R-squared:  0.9886\nF-statistic: 332.9 on 11 and 31 DF,  p-value: &lt; 2.2e-16\n</code></pre></p> <p>I don't know about you, but I think this is mega. However, we\u2019ll take it a step further and write our own <code>Python</code> code \u2014 mostly from scratch \u2014 to calculate these statistics ourselves and better understand what they mean.</p>"},{"location":"linear_regression.html#example-from-python","title":"Example from Python","text":""},{"location":"linear_regression.html#using-a-class-to-reproduce-the-r-statistics","title":"Using a class to reproduce the <code>R</code> statistics","text":"<p>In this example, we will implement a <code>LinearRegression</code> class in Python that performs linear regression and reproduces the same statistics that the <code>R</code> <code>summary</code> function outputs.</p> <p>To explore this example, navigate to the folder <code>./codes/</code> in the repository. Activate the virtual environment we created earlier (see Requirements).</p> <p>Launch Jupyter Notebook:</p> console<pre><code>jupyter notebook\n</code></pre> <p>Open the notebook named <code>linear_regression_py.ipynb</code>, which contains <code>Python</code> code. Make sure you use the <code>Python 3</code> kernel. </p> <p>We start by importing <code>pandas</code> for data handling and the <code>LinearRegression</code> class from our module:</p> Python<pre><code>import pandas as pd\nfrom module.lin_reg import LinearRegression \n</code></pre> <p>The <code>pandas</code> library makes it easy to load tabular datasets, such as the <code>USJudgeRatings</code> data stored in a CSV file:</p> Python<pre><code>df = pd.read_csv(\"data/USJudgeRatings.csv\", index_col=0)\nprint(df.head())\n</code></pre> <p>To prepare the data, we convert the <code>DataFrame</code> to NumPy arrays:</p> Python<pre><code>X = df.to_numpy()[:, :-1]\ny = df.to_numpy()[:, -1]\n</code></pre> <p>Then, we create the <code>LinearRegression</code> instance and fit the data:</p> Python<pre><code>model = LinearRegression()\n_ = model.fit(X,y)\n</code></pre> <p>To display the regression results, call the <code>summary()</code> method::</p> Python<pre><code>model.summary()\n</code></pre> <p>This produces the same statistics as the <code>R</code> summary:</p> Output<pre><code>Residuals:\nMin: -0.2212\nQ1:  -0.0615\nMed: -0.0105\nQ3:  0.0505\nMax: 0.2608\n\n Coefficient   Std Error     t-value     p-value\n     -2.1194      0.5190     -4.0834   0.0002896\n      0.0128      0.0259      0.4947      0.6243\n      0.3648      0.1294      2.8204    0.008291\n      0.1254      0.0897      1.3978      0.1721\n      0.0667      0.1430      0.4663      0.6443\n     -0.1945      0.1478     -1.3163      0.1977\n      0.2783      0.1383      2.0129     0.05288\n     -0.0020      0.2400     -0.0082      0.9935\n     -0.1358      0.2672     -0.5081       0.615\n      0.5478      0.2772      1.9759     0.05712\n     -0.0681      0.3148     -0.2162      0.8303\n      0.2688      0.0621      4.3263   0.0001464\n\nResidual standard error: 0.1174\nR-squared:             0.9916\nAdjusted R-squared:    0.9886\nF-statistic:           332.8597\nF-statistic p-value:   1.11e-16\n</code></pre> <p>Next, we will unpack the <code>LinearRegression</code> class to see how it works!</p>"},{"location":"linear_regression.html#the-linearregression-class","title":"The <code>LinearRegression</code> class","text":"<p>All Python source code for this example is stored in the folder <code>./codes/modules/</code>.</p> <ul> <li>The file <code>__init__.py</code> marks the directory as a Python package, allowing you to import its modules elsewhere in your project.</li> <li>The file <code>lin_reg.py</code> defines the <code>LinearRegression</code> class, which implements our regression model and associated methods.</li> <li>The file <code>test_lin_reg.py</code> contains unit tests to verify that the implementation in <code>lin_reg.py</code> works correctly<sup>1</sup>.</li> </ul> <p>Every <code>Python</code> class can define an initialiser method (often called the constructor in other languages) using <code>__init__</code>. This special method is automatically executed when a new instance of the class is created<sup>2</sup>.</p> Python<pre><code>class LinearRegression:\n    \"\"\"\n    Ordinary Least Squares (OLS) linear regression using the normal equation.\n    For full details, see the complete class docstring in `lin_reg.py`\n    \"\"\"\n\n    def __init__(self, add_bias: bool = True):\n        \"\"\"\n        Initializes the model.\n\n        Parameters\n        ----------\n        add_bias : bool, optional (default=True)\n            If True, automatically adds a column of ones to X to estimate\n            an intercept term (\u03b2\u2080). If False, the model is fit without an\n            intercept (\u03b2\u2080 fixed at 0).\n        \"\"\"\n        self.add_bias = add_bias # Instance attribute: model configuration\n        self.beta = None  # Instance attribute: will store the estimated coefficients after fitting\n\n        self.X = None # Instance attribute: will store the independent variables of the training set\n        self.y = None # Instance attribute: will store the dependent variable of the training set\n</code></pre> <ol> <li> <p>Testing is beyond the scope of this tutorial, but it is an essential skill in software development. Automated tests help ensure that your code produces the expected results, prevent regressions when you make changes, and improve confidence in the correctness of your program.\u00a0\u21a9</p> </li> <li> <p>In general, I will reduce the docstrings for presentation here. You can gain additional context by looking through the docstrings in <code>lin_reg.py</code>.\u00a0\u21a9</p> </li> </ol>"},{"location":"requirements.html","title":"Requirements","text":"<p>First off, I would recommend using a Conda virual environment:</p> <pre><code>conda create --name ML_train\nconda activate ML_train\n</code></pre> <pre><code>conda install -c conda-forge python ipython jupyterlab \nconda install -c conda-forge scipy pandas numpy networkx scikit-learn\nconda install -c conda-forge nbconvert matplotlib seaborn plotly\nconda install -c conda-forge streamlit scikit-plot sqlite grip\npip3 install torch torchvision torchaudio\npip3 install mkdocs mkdocs-material\npip3 install mkdocs-pymdownx-material-extras\nconda install -c conda-forge r-base r-essentials r-pacman r-psychtools r-lars\n</code></pre> <p>We probably don't need all of these packages, but this is the environment I'm working with. You can also try:</p> <pre><code>    conda env create -f requirements_mac.yml\n</code></pre> <p>using the requirements file in the main directory. I have typically found that this only works on the same OS though.</p>"},{"location":"requirements.html#some-notes","title":"Some notes","text":"<ul> <li><code>scikit-learn</code> and <code>torch</code> are for machine learning.</li> <li><code>sqlite</code> is used to manipulate SQL databases using SQL in Python. (Very useful data structures for industry).</li> <li><code>mkdocs</code> is a great package for making code documentation websites (Like this one!).</li> <li><code>nbconvert</code> converts Jupyter notebooks to .py files. </li> <li><code>matplotlib</code>, <code>seaborn</code>, <code>plotly</code> is for plotting and data visualisation, <code>streamlit</code> is for dashboards.</li> <li><code>r-base</code>, <code>r-essentials</code>, <code>r-pacman</code> will let us use <code>R</code>. <code>r-pacman</code> is a package manager that will automatically download required packages from CRAN. For deployment, it might be better to download packages using <code>conda</code>. This will work fine for us though.</li> <li><code>r-psychtools</code> has the package <code>psych</code> which contains useful functions like <code>describe()</code>. On my Mac, I needed to have XCode installed first for this to work.</li> </ul>"}]}