{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Machine Learning Tutorials","text":""},{"location":"index.html#introduction","title":"\ud83d\udcd8 Introduction","text":"<p>Welcome! This repository is a personal project designed to demonstrate machine learning techniques and explore tools from the data science and software development ecosystem.</p> <p>The code and notebooks are structured as hands-on tutorials, so feel free to follow along, experiment, and build on them.</p> <p>To get started, please set up your virtual environment. Instructions can be found on the Requirements page.</p>"},{"location":"index.html#supervised-learning","title":"\ud83e\udd16 Supervised Learning","text":"<p>Supervised learning is a machine learning approach where a model is trained on labeled data\u2014that is, input-output pairs. The goal is to learn a general rule that maps inputs to outputs. This type of learning is a form of predictive modelling, where models are trained on historical data to make predictions about new, unseen data.</p>"},{"location":"index.html#examples-of-supervised-learning","title":"\ud83d\udd0d Examples of Supervised Learning","text":"<ul> <li> <p>Forecasting energy consumption   A model is trained on a dataset containing daily energy usage over the course of a year. It then predicts energy usage for future days based on patterns it has learned.   \u2192 This is an example of regression, because the target output (energy usage) is a continuous variable.</p> </li> <li> <p>Email spam detection   A model is trained on a labeled dataset of emails, where each email is marked as spam or not spam. The model learns to classify new, unseen emails accordingly.   \u2192 This is an example of classification, because the target output is a categorical variable (e.g., {\"spam\", \"not spam\"} or {0, 1}).</p> </li> </ul>"},{"location":"index.html#regression","title":"\ud83d\udcc8 Regression","text":"<p>Regression analysis refers to a family of methods used to estimate the relationship between a continuous dependent variable (also called the output, response, or label) and one or more independent variables (also called features, predictors, or regressors).</p>"},{"location":"index.html#regression-examples","title":"\ud83d\udd0d Regression Examples","text":"<ul> <li>Linear Regression</li> <li>(More examples coming soon)</li> </ul>"},{"location":"index.html#classification","title":"\ud83e\uddee Classification","text":"<p>Classification is the task of predicting categorical labels from input data. The model learns to assign new data points to one of a finite set of classes.</p>"},{"location":"index.html#classification-examples","title":"\ud83d\udd0d Classification Examples","text":"<ul> <li>Logistic Regression</li> <li>(More examples coming soon)</li> </ul>"},{"location":"linear_regression.html","title":"Linear Regression","text":""},{"location":"linear_regression.html#introduction","title":"Introduction","text":"<p>Linear regression is a type of supervised learning, where the goal is to identify a best-fit function between a dependent variable and one or more independent variables.</p>"},{"location":"linear_regression.html#model-definition","title":"Model Definition","text":"<p>We assume the model takes the form:</p> \\[ y_i = \\beta_0 + \\beta_1 x_{i,1} + \\dots + \\beta_N x_{i,N} + \\epsilon_i, \\] <p>where \\( i = 1, \\dots, M \\) indexes the samples in the training set.</p> <p>The components of the model are:</p> <ul> <li>\\( x_{i,j} \\) \u2014 Feature \\( j \\) for sample \\( i \\); there are \\( N \\) features.</li> <li>\\( y_i \\) \u2014 The output (target) for sample \\( i \\).</li> <li>\\( \\epsilon_i \\) \u2014 The error term, representing noise or unexplained variation.</li> <li>\\( \\beta_0 \\) \u2014 The intercept (bias).</li> <li>\\( \\beta_j \\) \u2014 The coefficient for feature \\( j \\).</li> </ul>"},{"location":"linear_regression.html#fitting-the-model-ordinary-least-squares-ols","title":"Fitting the Model: Ordinary Least Squares (OLS)","text":"<p>The most common way to fit a linear regression model is with the ordinary least squares (OLS) method. The objective is to find parameters \\( \\boldsymbol{\\beta} \\) that minimise the following cost function:</p> \\[ J(\\boldsymbol{\\beta}) = \\frac{1}{2} \\sum_{i=1}^{M} \\left(h_{\\boldsymbol{\\beta}}(X_i) - y_i\\right)^2, \\] <p>where:</p> <ul> <li>\\( \\boldsymbol{\\beta} = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_N \\end{bmatrix} \\) \u2014 Vector of model parameters.</li> <li>\\( X_i = \\begin{bmatrix} 1 &amp; x_{i,1} &amp; x_{i,2} &amp; \\dots &amp; x_{i,N} \\end{bmatrix} \\) \u2014 Row vector of features for sample \\( i \\), including a 1 to account for the bias term \\( \\beta_0 \\).</li> <li>\\( h_{\\boldsymbol{\\beta}}(X_i) \\) \u2014 The hypothesis (model) function, defined as:</li> </ul> \\[ h_{\\boldsymbol{\\beta}}(X_i) = \\beta_0 + \\beta_1 x_{i,1} + \\dots + \\beta_N x_{i,N} = \\boldsymbol{\\beta}^\\top X_i^\\top. \\] <p>Thus, we are minimising the sum of squared differences between the predicted outputs \\( h_{\\boldsymbol{\\beta}}(X_i) \\) and the actual outputs \\( y_i \\) across all training samples.</p>"},{"location":"linear_regression.html#closed-form-solution","title":"Closed-Form Solution","text":"<p>To simplify notation and enable vectorised computation, we define:</p> <ul> <li>The design matrix:</li> </ul> \\[ X = \\begin{bmatrix} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_M \\end{bmatrix} \\quad \\text{(an \\( M \\times (N+1) \\) matrix)} \\] <ul> <li>The output vector:</li> </ul> \\[ \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_M \\end{bmatrix} \\] <p>OLS is one of the few machine learning methods with a closed-form analytic solution for the best-fit parameters. The solution is:</p> \\[ \\boldsymbol{\\beta} = (X^\\top X)^{-1} X^\\top \\mathbf{y} \\] <p>This gives the value of \\( \\boldsymbol{\\beta} \\) that minimises the cost function and fits the training data as closely as possible in the least-squares sense.</p>"},{"location":"linear_regression.html#example-from-r","title":"Example from <code>R</code>","text":"<p>The programming language <code>R</code> is widely used in statistics and data visualisation. It also provides a useful library of built-in datasets.</p> <p>To explore this, navigate to the folder <code>./codes/</code> in the repository. Activate the virtual environment we created earlier (see Requirements).</p> <p>Then, launch Jupyter Notebook:</p> console<pre><code>jupyter notebook\n</code></pre> <p>Open the notebook named <code>linear_regression_R.ipynb</code>, which contains <code>R</code> code. Make sure you use the <code>R</code> kernel. </p> <p>We begin by importing <code>datasets</code> package:</p> R<pre><code>library(datasets)\n</code></pre> <p>This package includes several classic datasets, one of which is <code>USJudgeRatings</code>. This dataset contains average ratings for 43 judges across several dimensions (e.g., integrity, diligence, writing quality). The 12th column is the rating for \u201cworthy of retention,\u201d which we\u2019ll try to predict using the first 11 features.</p> <p>This command prints the first six rows of the dataset:</p> R<pre><code>dataset = USJudgeRatings\nhead(dataset)\n</code></pre> US Judge Ratings CONTINTGDMNRDILGCFMGDECIPREPFAMIORALWRITPHYSRTEN AARONSON,L.H.5.77.97.77.37.17.47.17.17.17.08.37.8 ALEXANDER,J.M.6.88.98.88.57.88.18.08.07.87.98.58.7 ARMENTANO,A.J.7.28.17.87.87.57.67.57.57.37.47.97.8 BERDON,R.I.6.88.88.58.88.38.58.78.78.48.58.88.7 BRACKEN,J.J.7.36.44.36.56.06.25.75.75.15.35.54.8 BURNS,E.B.6.28.88.78.57.98.08.18.08.08.08.68.6 <p>To prepare the data and run linear regression:</p> R<pre><code>x &lt;- as.matrix(dataset[, c(1:11)]) # First 11 dimensions are Features\ny &lt;- as.matrix(dataset[, 12]) # last dimeions is the Output (Retention rating)\nreg &lt;- lm(y ~ x)\n</code></pre> <p>This creates matrices <code>x</code> and <code>y</code> and uses the <code>lm</code> function to perform linear regression.</p> <p>To see the model summary:</p> R<pre><code>summary(reg)\n</code></pre> <p>This produces several useful statistics: Output<pre><code>Call:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max\n-0.22123 -0.06155 -0.01055  0.05045  0.26079\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -2.11943    0.51904  -4.083 0.000290 ***\nxCONT        0.01280    0.02586   0.495 0.624272\nxINTG        0.36484    0.12936   2.820 0.008291 **\nxDMNR        0.12540    0.08971   1.398 0.172102\nxDILG        0.06669    0.14303   0.466 0.644293\nxCFMG       -0.19453    0.14779  -1.316 0.197735\nxDECI        0.27829    0.13826   2.013 0.052883 .\nxPREP       -0.00196    0.24001  -0.008 0.993536\nxFAMI       -0.13579    0.26725  -0.508 0.614972\nxORAL        0.54782    0.27725   1.976 0.057121 .\nxWRIT       -0.06806    0.31485  -0.216 0.830269\nxPHYS        0.26881    0.06213   4.326 0.000146 ***\n---\nSignif. codes:  0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1\n\nResidual standard error: 0.1174 on 31 degrees of freedom\nMultiple R-squared:  0.9916,    Adjusted R-squared:  0.9886\nF-statistic: 332.9 on 11 and 31 DF,  p-value: &lt; 2.2e-16\n</code></pre></p> <p>I don't know about you, but I think this is mega. However, we\u2019ll take it a step further and write our own <code>Python</code> code \u2014 mostly from scratch \u2014 to calculate these statistics ourselves and better understand what they mean.</p>"},{"location":"linear_regression.html#example-from-python","title":"Example from Python","text":""},{"location":"linear_regression.html#using-a-class-to-reproduce-the-r-statistics","title":"Using a class to reproduce the <code>R</code> statistics","text":"<p>In this example, we will implement a <code>LinearRegression</code> class in Python that performs linear regression and reproduces the same statistics that the <code>R</code> <code>summary</code> function outputs.</p> <p>To explore this example, navigate to the folder <code>./codes/</code> in the repository. Activate the virtual environment we created earlier (see Requirements).</p> <p>Launch Jupyter Notebook:</p> console<pre><code>jupyter notebook\n</code></pre> <p>Open the notebook named <code>linear_regression_py.ipynb</code>, which contains <code>Python</code> code. Make sure you use the <code>Python 3</code> kernel. </p> <p>We start by importing <code>pandas</code> for data handling and the <code>LinearRegression</code> class from our module:</p> Python<pre><code>import pandas as pd\nfrom modules.lin_reg import LinearRegression \n</code></pre> <p>The <code>pandas</code> library makes it easy to load tabular datasets, such as the <code>USJudgeRatings</code> data stored in a CSV file:</p> Python<pre><code>df = pd.read_csv(\"data/USJudgeRatings.csv\", index_col=0)\nprint(df.head())\n</code></pre> <p>To prepare the data, we convert the <code>DataFrame</code> to NumPy arrays:</p> Python<pre><code>X = df.to_numpy()[:, :-1]\ny = df.to_numpy()[:, -1]\n</code></pre> <p>Then, we create the <code>LinearRegression</code> instance and fit the data:</p> Python<pre><code>model = LinearRegression()\n_ = model.fit(X,y)\n</code></pre> <p>To display the regression results, call the <code>summary()</code> method::</p> Python<pre><code>model.summary()\n</code></pre> <p>This produces the same statistics as the <code>R</code> summary:</p> Output<pre><code>Residuals:\nMin: -0.2212\nQ1:  -0.0615\nMed: -0.0105\nQ3:  0.0505\nMax: 0.2608\n\n Coefficient   Std Error     t-value     p-value\n     -2.1194      0.5190     -4.0834   0.0002896\n      0.0128      0.0259      0.4947      0.6243\n      0.3648      0.1294      2.8204    0.008291\n      0.1254      0.0897      1.3978      0.1721\n      0.0667      0.1430      0.4663      0.6443\n     -0.1945      0.1478     -1.3163      0.1977\n      0.2783      0.1383      2.0129     0.05288\n     -0.0020      0.2400     -0.0082      0.9935\n     -0.1358      0.2672     -0.5081       0.615\n      0.5478      0.2772      1.9759     0.05712\n     -0.0681      0.3148     -0.2162      0.8303\n      0.2688      0.0621      4.3263   0.0001464\n\nResidual standard error: 0.1174\nR-squared:             0.9916\nAdjusted R-squared:    0.9886\nF-statistic:           332.8597\nF-statistic p-value:   1.11e-16\n</code></pre> <p>Next, we will unpack the <code>LinearRegression</code> class to see how it works!</p>"},{"location":"linear_regression.html#the-linearregression-class","title":"The <code>LinearRegression</code> class","text":"<p>All Python source code for this example is stored in the folder <code>./codes/modules/</code>.</p> <ul> <li>The file <code>__init__.py</code> marks the directory as a Python package, allowing you to import its modules elsewhere in your project.</li> <li>The file <code>lin_reg.py</code> defines the <code>LinearRegression</code> class, which implements our regression model and associated methods.</li> <li>The file <code>test_lin_reg.py</code> contains unit tests to verify that the implementation in <code>lin_reg.py</code> works correctly<sup>1</sup>.</li> </ul> <p>We start by importing the required libraries:</p> Python<pre><code>import numpy as np\nfrom scipy import stats\nfrom typing import Tuple\nfrom numpy.typing import NDArray\n</code></pre> <p>Every <code>Python</code> class can define an initialiser method (often called the constructor in other languages) using <code>__init__</code>. This special method is automatically executed when a new instance of the class is created<sup>2</sup>.</p> Python<pre><code>class LinearRegression:\n    \"\"\"Ordinary Least Squares (OLS) linear regression using the normal equation.\"\"\"\n\n    def __init__(self, add_bias: bool = True):\n        self._add_bias = add_bias # Whether to include an intercept \u03b2\u2080\n        self.beta = None # Will hold fitted coefficients \n        self.X = None # Stores training features (with bias if added) \n        self.y = None # Stores training target values\n</code></pre>"},{"location":"linear_regression.html#fitting-the-model","title":"Fitting the model","text":"<p>The <code>fit</code> method computes the OLS solution:</p> <ul> <li>Adds a column of ones to <code>X</code> if an intercept term is included.</li> <li>Computes the regression coefficients using the pseudoinverse of \\(X^T X\\).</li> <li>Stores <code>X</code>, <code>y</code> and the fitted coefficients in the instance for later use.</li> </ul> Python<pre><code>def fit(self, X: NDArray[np.float64], y: NDArray[np.float64]) -&gt; NDArray[np.float64]:\n    if self._add_bias:\n        ones = np.ones((X.shape[0], 1))\n        X = np.hstack((ones, X))\n\n    XtX_inv = np.linalg.pinv(X.T @ X)\n    self.beta = XtX_inv @ X.T @ y\n\n    self.X = X\n    self.y = y\n\n    return self.beta\n</code></pre>"},{"location":"linear_regression.html#making-predictions-train-vs-test","title":"Making predictions (train vs. test)","text":"<p>The learned relationship lives entirely in the fitted coefficients \\( \\boldsymbol{\\beta} \\) that are computed during <code>fit(X_train, y_train)</code>. Once <code>beta</code> is set, you can call <code>predict(X)</code> on any dataset that has the same feature schema:</p> <ul> <li>Training predictions: <code>predict(X_train)</code></li> <li>Test/Validation predictions: <code>predict(X_test)</code></li> </ul> <p>Mathematically, predictions are always $$ \\hat{\\mathbf{y}} = X\\,\\boldsymbol{\\beta}. $$</p> <p>If the model was created with <code>add_bias=True</code>, <code>predict</code> will automatically prepend the column of ones to whatever <code>X</code> you pass in. It then delegates to the internal <code>_predict</code>, which assumes <code>X</code> is already in final matrix form (bias included if needed).</p> <p>Note: Do not re-fit on test data. Fit once on training data, then reuse the same beta to evaluate on validation/test sets (using <code>predict</code>).</p> Python<pre><code>def predict(self, X: NDArray[np.float64]) -&gt; NDArray[np.float64]:\n    if self.beta is None:\n        raise ValueError(\"Model has not been fitted yet.\")\n\n    if self._add_bias:\n        ones = np.ones((X.shape[0], 1))\n        X = np.hstack((ones, X))\n\n    return self._predict(X)\n\ndef _predict(self, X: NDArray[np.float64]) -&gt; NDArray[np.float64]:\n    return X @ self.beta\n</code></pre>"},{"location":"linear_regression.html#computing-residuals-train-vs-test","title":"Computing residuals (train vs. test)","text":"<p>Residuals are the differences between observed targets and predictions: $$ \\mathbf{r} = \\mathbf{y} - \\hat{\\mathbf{y}} = \\mathbf{y} - X\\boldsymbol{\\beta} $$</p> <p>Use the same <code>residuals(X, y)</code> method for either training or test data:</p> <ul> <li>Training residuals: <code>residuals(X_train, y_train)</code></li> <li>Test/validation residuals:<code>residuals(X_test, y_test)</code></li> </ul> <p>As with <code>predict</code>, if <code>add_bias=True</code>, the public <code>residuals</code> method will add the bias column for you before delegating to <code>_residuals</code>. The private <code>_residuals</code> assumes X is already prepared.</p> Python<pre><code>def residuals(self, X: NDArray[np.float64], y: NDArray[np.float64]) -&gt; NDArray[np.float64]:\n    if self.beta is None:\n        raise ValueError(\"Model has not been fitted yet.\")\n\n    if self._add_bias:\n        ones = np.ones((X.shape[0], 1))\n        X = np.hstack((ones, X))\n\n    return self._residuals(X, y)\n\ndef _residuals(self, X: NDArray[np.float64], y: NDArray[np.float64]) -&gt; NDArray[np.float64]:\n    y_pred = self._predict(X)\n    return y - y_pred\n</code></pre>"},{"location":"linear_regression.html#calculating-statistics","title":"Calculating Statistics","text":""},{"location":"linear_regression.html#residual-summary-statistics","title":"Residual summary statistics","text":"<p>The <code>residual_stats</code> method returns the five-number summary of the model residuals:</p> <ul> <li>Minimum: the smallest residual value.</li> <li>Q1 (25th percentile): the value below which 25% of residuals lie.</li> <li>Median: the middle value (50th percentile) of the residuals.</li> <li>Q3 (75th percentile): the value below which 75% of residuals lie.</li> <li>Maximum: the largest residual value.</li> </ul> <p>These statistics are useful for identifying skewness or outliers in the residual distribution. They mirror the residual summary output seen in statistical software like R.</p> <p>Mathematically, if \\(\\mathbf{r} = \\mathbf{y} - \\hat{\\mathbf{y}}\\), then:</p> \\[ \\text{summary}(\\mathbf{r}) = \\big[ \\min(\\mathbf{r}), Q_1(\\mathbf{r}), \\operatorname{median}(\\mathbf{r}), Q_3(\\mathbf{r}), \\max(\\mathbf{r}) \\big] \\] Python<pre><code>def residual_stats(self) -&gt; np.ndarray:\n    if self.beta is None:\n        raise ValueError(\"Model has not been fitted yet.\")\n\n    # Compute residuals using stored training data\n    residuals = self._residuals(self.X, self.y)\n\n    # Compute and return five-number summary as a NumPy array\n    return np.array([\n        np.min(residuals),\n        np.percentile(residuals, 25),\n        np.median(residuals),\n        np.percentile(residuals, 75),\n        np.max(residuals)\n    ])\n</code></pre>"},{"location":"linear_regression.html#residual-standard-error-rse","title":"Residual Standard Error (RSE)","text":"<p>The Residual Standard Error (RSE) measures the typical size of the residuals \u2014 in other words, how far the model's predictions are from the observed values on average.</p> <p>It is defined as: $$ \\mathrm{RSE} = \\sqrt{\\frac{\\mathrm{RSS}}{n - p}} $$</p> <p>where:</p> <ul> <li>\\(\\mathrm{RSS} = \\sum_{i=1}^n r_i^2\\) is the Residual Sum of Squares,</li> <li>\\(n\\) is the number of observations,</li> <li>\\(p\\) is the number of estimated parameters (including the intercept).</li> </ul> <p>This formula is equivalent to taking the square root of the estimated error variance: $$ \\hat{\\sigma}^2 = \\frac{\\mathrm{RSS}}{n - p} \\quad\\Rightarrow\\quad \\mathrm{RSE} = \\sqrt{\\hat{\\sigma}^2} $$</p> <p>The RSE is expressed in the same units as the dependent variable, making it easy to interpret: a smaller RSE means the model predictions are closer to the observed values.</p> Python<pre><code>def residuals_SE(self) -&gt; float:\n    if self.beta is None:\n        raise ValueError(\"Model has not been fitted yet.\")\n\n    # Compute residuals using stored data\n    residuals = self._residuals(self.X, self.y)\n\n    # Number of observations and parameters\n    n = len(residuals)\n    p = len(self.beta)\n\n    # Compute residual sum of squares and standard error\n    RSS = np.sum(residuals ** 2)\n    RSE = np.sqrt(RSS / (n - p))\n\n    return RSE\n</code></pre>"},{"location":"linear_regression.html#coefficient-of-determination-r2-and-adjusted-r2","title":"Coefficient of Determination: \\(R^2\\) and adjusted \\(R^2\\)","text":"<p>The coefficient of determination \\(R^2\\) measures the proportion of variability in the dependent variable that is explained by the model:</p> \\[ R^2 = 1 - \\frac{\\mathrm{RSS}}{\\mathrm{TSS}} \\] <p>where:</p> <ul> <li>\\(\\mathrm{RSS} = \\sum_{i=1}^n r_i^2\\) is the Residual Sum of Squares (unexplained variance),</li> <li>\\(\\mathrm{TSS} = \\sum_{i=1}^n (y_i - \\bar{y})^2\\) is the Total Sum of Squares (total variance in the data).</li> </ul> <p>While \\(R^2\\) indicates goodness of fit, it always increases when more predictors are added \u2014 even if they are not useful. To account for this, we compute the adjusted \\(R^2\\):</p> \\[ R^2_{\\text{adj}} = 1 - \\frac{\\mathrm{RSS}/(n - p)}{\\mathrm{TSS}/(n - 1)} \\] <p>where:</p> <ul> <li>\\(n\\) = number of observations,</li> <li>\\(p\\) = number of estimated parameters (including the intercept).</li> </ul> <p>Adjusted \\(R^2\\) penalizes the inclusion of unnecessary predictors, making it a better measure for comparing models with different numbers of features.</p> Python<pre><code>def R_squared(self) -&gt; Tuple[float, float]:\n    if self.beta is None:\n        raise ValueError(\"Model has not been fitted yet.\")\n\n    # Compute residuals using stored training data\n    residuals = self._residuals(self.X, self.y)\n\n    # Number of observations and estimated parameters\n    n = len(residuals)\n    p = len(self.beta)\n\n    # Residual Sum of Squares (unexplained variance)\n    RSS = np.sum(residuals ** 2)\n\n    # Total Sum of Squares (total variance in y)\n    TSS = np.sum((self.y - np.mean(self.y)) ** 2)\n\n    # R\u00b2 = 1 - RSS/TSS\n    R_squared = 1 - RSS / TSS\n\n    # Adjusted R\u00b2 penalizes for model complexity\n    R_squared_adj = 1 - (RSS / (n - p)) / (TSS / (n - 1))\n\n    return R_squared, R_squared_adj\n</code></pre>"},{"location":"linear_regression.html#overall-significance-the-f-stat-and-p-value","title":"Overall significance: the F-stat and p-value","text":"<p>The \\(F\\)-statistic tests the null hypothesis that all regression coefficients except the intercept are equal to zero:</p> \\[ H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_{p-1} = 0 \\] <p>In other words, it checks whether the model provides a better fit than one with only the intercept.</p> <ul> <li> <p>Calculate MSR and MSE: Let \\(\\mathrm{TSS}\\) be the Total Sum of Squares and \\(\\mathrm{RSS}\\) be the Residual Sum of Squares:</p> <ul> <li>Mean Square Regression (MSR) \u2014 average explained variance per parameter:   $$   \\mathrm{MSR} = \\frac{\\mathrm{TSS} - \\mathrm{RSS}}{df_1}   $$   where:<ul> <li>\\(df_1 = p - 1\\) (if <code>self._add_bias=True</code>) </li> <li>\\(df_1 = p\\) (if <code>self._add_bias = False</code>).</li> </ul> </li> <li>Mean Square Error (MSE) \u2014 average unexplained variance per residual degree of freedom:   $$   \\mathrm{MSE} = \\frac{\\mathrm{RSS}}{df_2}   $$   where \\(df_2 = n-p\\).</li> </ul> </li> <li> <p>Calculate  \\(F\\)-statistic and p-value:      The \\(F\\)-statistic is the ratio:     $$     F = \\frac{\\mathrm{MSR}}{\\mathrm{MSE}}     $$</p> <p>A large \\(F\\)-value suggests that the model explains significantly more variance than would be expected by chance. The p-value is computed from the right tail of the \\(F\\)-distribution with \\((df_1, df_2)\\) degrees of freedom. The one-tailed p-value is: $$ p = \\left( 1 - \\text{CDF}_{df_1, df_2}(F)\\right). $$</p> </li> <li> <p>Interpretation:</p> <ul> <li>Small p-value (reject \\(H_0\\)): at least one predictor is significantly associated with the response.</li> <li>Large p-value (fail to reject \\(H_0\\)): no evidence the predictors improve the model.</li> </ul> </li> </ul> Python<pre><code>def F_score(self) -&gt; Tuple[float, float]:\n    if self.beta is None:\n        raise ValueError(\"Model has not been fitted yet.\")\n\n    # Compute residuals using stored training data\n    residuals = self._residuals(self.X, self.y)\n\n    # Number of observations and number of estimated parameters\n    n = len(residuals)\n    p = len(self.beta)\n\n    # Residual Sum of Squares (RSS) \u2014 unexplained variation\n    RSS = np.sum(residuals ** 2)\n\n    # Total Sum of Squares (TSS) \u2014 total variation in y\n    TSS = np.sum((self.y - np.mean(self.y)) ** 2)\n\n    # Degrees of freedom\n    if self._add_bias:\n        df1 = p - 1            # Numerator degrees of freedom (model)\n    else:\n        df1 = p\n    df2 = n - p            # Denominator degrees of freedom (residuals)\n\n    # Mean Square Regression and Mean Square Error\n    MSR = (TSS - RSS) / df1  # Explained variance per parameter\n    MSE = RSS / df2          # Unexplained variance per residual degree of freedom\n\n    # F-statistic: ratio of explained to unexplained variance\n    F_stat = MSR / MSE\n\n    # p-value from the F-distribution (right-tailed test)\n    p_value = 1 - stats.f.cdf(F_stat, df1, df2)\n\n    return F_stat, p_value\n</code></pre>"},{"location":"linear_regression.html#standard-errors-of-coefficients","title":"Standard Errors of Coefficients","text":"<p>The standard error of each regression coefficient measures the variability of its estimate across hypothetical repeated samples. It comes from the diagonal entries of the variance\u2013covariance matrix of \\( \\boldsymbol{\\beta} \\).</p> <p>The method works as follows:</p> <ul> <li> <p>Get residuals from stored training data:    We call the internal <code>_residuals(self.X, self.y)</code> to compute    $$    \\mathbf{r} = \\mathbf{y} - \\hat{\\mathbf{y}}.    $$</p> </li> <li> <p>Compute \\((X^\\top X)^{-1}\\):    This matrix appears in the closed-form OLS solution and is needed to propagate uncertainty into the coefficient estimates.</p> </li> <li> <p>Estimate the variance of the residuals:    Using    $$    \\hat{\\sigma}^2 = \\frac{\\text{RSS}}{n - p}    $$    where:</p> <ul> <li>\\(\\text{RSS} = \\sum_i r_i^2\\) is the residual sum of squares,</li> <li>\\(n\\) is the number of observations,</li> <li>\\(p\\) is the number of estimated parameters (including the intercept).</li> </ul> </li> <li> <p>Form the variance\u2013covariance matrix of \\(\\boldsymbol{\\beta}\\):    $$    \\text{Var}(\\boldsymbol{\\beta}) = \\hat{\\sigma}^2 \\, (X^\\top X)^{-1}.    $$</p> </li> <li> <p>Extract standard errors:    Take the square root of each diagonal element to get    $$    \\text{SE}(\\beta_j) = \\sqrt{ \\text{Var}(\\beta_j) }.    $$</p> </li> </ul> <p>These standard errors are crucial for computing t-statistics and p-values when testing the significance of each coefficient.</p> Python<pre><code>def coefficients_SE(self) -&gt; NDArray[np.float64]:\n    if self.beta is None:\n        raise ValueError(\"Model has not been fitted yet.\")\n\n    # Compute residuals from stored data\n    residuals = self._residuals(self.X, self.y)\n\n    # Compute (X^T X)^(-1)\n    XtX_inv = np.linalg.pinv(self.X.T @ self.X)\n\n    # Compute estimated variance of errors\n    n = len(residuals)\n    p = len(self.beta)\n    RSS = np.sum(residuals ** 2)\n    sigma_squared = RSS / (n - p)\n\n    # Compute variance-covariance matrix of beta\n    var_beta = sigma_squared * XtX_inv\n\n    # Standard errors are the square roots of the diagonal entries\n    coeff_RSE = np.sqrt(np.diag(var_beta))\n\n    return coeff_RSE\n</code></pre>"},{"location":"linear_regression.html#t-stats-and-p-values-for-coefficients","title":"t-stats and p-values for Coefficients","text":"<p>Once we have the standard errors of the regression coefficients, we can test whether each coefficient is statistically different from zero.</p> <ul> <li> <p>Compute t-statistics    For each coefficient \\( \\beta_j \\), the t-statistic is computed as:    $$    t_j = \\frac{\\beta_j}{\\mathrm{SE}(\\beta_j)}    $$    This measures how many standard errors the coefficient is away from zero.</p> </li> <li> <p>Determine degrees of freedom    We use:    $$    \\text{df} = n - p    $$    where:</p> <ul> <li>\\(n\\) is the number of observations,</li> <li>\\(p\\) is the number of parameters estimated (including the intercept).</li> </ul> </li> <li> <p>Compute two-tailed p-values    Under the null hypothesis \\(H_0 : \\beta_j = 0\\), the t-statistic follows a Student\u2019s t-distribution with \\(n - p\\) degrees of freedom.    The two-tailed p-value is:    $$    p_j = 2 \\left( 1 - F_t\\left( \\left| t_j \\right| \\right) \\right)    $$    where \\(F_t\\) is the cumulative distribution function (CDF) of the t-distribution.</p> </li> </ul> <p>These p-values indicate the probability of observing such extreme \\(t\\)-statistics if the true coefficient were zero. Small p-values (commonly below 0.05) suggest that the coefficient is statistically significant.</p> Python<pre><code>def coefficients_p_values(self) -&gt; Tuple[NDArray[np.float64], NDArray[np.float64]]:\n    if self.beta is None:\n        raise ValueError(\"Model has not been fitted yet.\")\n\n    # Compute t-statistics: each beta divided by its standard error\n    t_values = self.beta / self.coefficients_SE()\n\n    # Number of observations and number of parameters\n    n = len(self.y)\n    p = len(self.beta)\n\n    # Compute two-tailed p-values using the t-distribution CDF\n    p_values = 2 * (1 - stats.t.cdf(np.abs(t_values), df=n - p))\n\n    return t_values, p_values\n</code></pre>"},{"location":"linear_regression.html#summary-method","title":"Summary method","text":"<p>Together, all of these statistics can be used to generate our own summary function. See <code>summary(self)</code> in <code>lin_reg.py</code>.</p> <ol> <li> <p>Testing is beyond the scope of this tutorial, but it is an essential skill in software development. Automated tests help ensure that your code produces the expected results, prevent regressions when you make changes, and improve confidence in the correctness of your program.\u00a0\u21a9</p> </li> <li> <p>In general, I will reduce the docstrings for presentation here. You can gain additional context by looking through the docstrings in <code>lin_reg.py</code>.\u00a0\u21a9</p> </li> </ol>"},{"location":"requirements.html","title":"Requirements","text":"<p>First off, I would recommend using a Conda virual environment:</p> <pre><code>conda create --name ML_train\nconda activate ML_train\n</code></pre> <pre><code>conda install -c conda-forge python ipython jupyterlab \nconda install -c conda-forge scipy pandas numpy networkx scikit-learn\nconda install -c conda-forge nbconvert matplotlib seaborn plotly\nconda install -c conda-forge streamlit scikit-plot sqlite grip\npip3 install torch torchvision torchaudio\npip3 install mkdocs mkdocs-material\npip3 install mkdocs-pymdownx-material-extras\nconda install -c conda-forge r-base r-essentials r-pacman r-psychtools r-lars\n</code></pre> <p>We probably don't need all of these packages, but this is the environment I'm working with. You can also try:</p> <pre><code>    conda env create -f requirements_mac.yml\n</code></pre> <p>using the requirements file in the main directory. I have typically found that this only works on the same OS though.</p>"},{"location":"requirements.html#some-notes","title":"Some notes","text":"<ul> <li><code>scikit-learn</code> and <code>torch</code> are for machine learning.</li> <li><code>sqlite</code> is used to manipulate SQL databases using SQL in Python. (Very useful data structures for industry).</li> <li><code>mkdocs</code> is a great package for making code documentation websites (Like this one!).</li> <li><code>nbconvert</code> converts Jupyter notebooks to .py files. </li> <li><code>matplotlib</code>, <code>seaborn</code>, <code>plotly</code> is for plotting and data visualisation, <code>streamlit</code> is for dashboards.</li> <li><code>r-base</code>, <code>r-essentials</code>, <code>r-pacman</code> will let us use <code>R</code>. <code>r-pacman</code> is a package manager that will automatically download required packages from CRAN. For deployment, it might be better to download packages using <code>conda</code>. This will work fine for us though.</li> <li><code>r-psychtools</code> has the package <code>psych</code> which contains useful functions like <code>describe()</code>. On my Mac, I needed to have XCode installed first for this to work.</li> </ul>"}]}