{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Machine Learning Tutorials","text":""},{"location":"index.html#introduction","title":"\ud83d\udcd8 Introduction","text":"<p>Welcome! This repository is a personal project designed to demonstrate machine learning techniques and explore tools from the data science and software development ecosystem.</p> <p>The code and notebooks are structured as hands-on tutorials, so feel free to follow along, experiment, and build on them.</p> <p>To get started, please set up your virtual environment. Instructions can be found on the Requirements page.</p>"},{"location":"index.html#supervised-learning","title":"\ud83e\udd16 Supervised Learning","text":"<p>Supervised learning is a machine learning approach where a model is trained on labeled data\u2014that is, input-output pairs. The goal is to learn a general rule that maps inputs to outputs. This type of learning is a form of predictive modelling, where models are trained on historical data to make predictions about new, unseen data.</p>"},{"location":"index.html#examples-of-supervised-learning","title":"\ud83d\udd0d Examples of Supervised Learning","text":"<ul> <li> <p>Forecasting energy consumption   A model is trained on a dataset containing daily energy usage over the course of a year. It then predicts energy usage for future days based on patterns it has learned.   \u2192 This is an example of regression, because the target output (energy usage) is a continuous variable.</p> </li> <li> <p>Email spam detection   A model is trained on a labeled dataset of emails, where each email is marked as spam or not spam. The model learns to classify new, unseen emails accordingly.   \u2192 This is an example of classification, because the target output is a categorical variable (e.g., {\"spam\", \"not spam\"} or {0, 1}).</p> </li> </ul>"},{"location":"index.html#regression","title":"\ud83d\udcc8 Regression","text":"<p>Regression analysis refers to a family of methods used to estimate the relationship between a continuous dependent variable (also called the output, response, or label) and one or more independent variables (also called features, predictors, or regressors).</p>"},{"location":"index.html#regression-examples","title":"\ud83d\udd0d Regression Examples","text":"<ul> <li>Linear Regression</li> <li>(More examples coming soon)</li> </ul>"},{"location":"index.html#classification","title":"\ud83e\uddee Classification","text":"<p>Classification is the task of predicting categorical labels from input data. The model learns to assign new data points to one of a finite set of classes.</p>"},{"location":"index.html#classification-examples","title":"\ud83d\udd0d Classification Examples","text":"<ul> <li>Logistic Regression</li> <li>(More examples coming soon)</li> </ul>"},{"location":"linear_regression.html","title":"Linear Regression","text":""},{"location":"linear_regression.html#introduction","title":"Introduction","text":"<p>Linear regression is a type of supervised learning, where the goal is to identify a best-fit function between a dependent variable and one or more independent variables.</p>"},{"location":"linear_regression.html#model-definition","title":"Model Definition","text":"<p>We assume the model takes the form:</p> \\[ y_i = \\beta_0 + \\beta_1 x_{i,1} + \\dots + \\beta_N x_{i,N} + \\epsilon_i, \\] <p>where \\( i = 1, \\dots, M \\) indexes the samples in the training set.</p> <p>The components of the model are:</p> <ul> <li>\\( x_{i,j} \\) \u2014 Feature \\( j \\) for sample \\( i \\); there are \\( N \\) features.</li> <li>\\( y_i \\) \u2014 The output (target) for sample \\( i \\).</li> <li>\\( \\epsilon_i \\) \u2014 The error term, representing noise or unexplained variation.</li> <li>\\( \\beta_0 \\) \u2014 The intercept (bias).</li> <li>\\( \\beta_j \\) \u2014 The coefficient for feature \\( j \\).</li> </ul> <p>Note: We typically use \\( \\beta \\) for both the true model parameters and the estimated parameters. It's common to overload the symbol and rely on context, though you may also see \\( \\hat{\\beta} \\) used for estimated values.</p>"},{"location":"linear_regression.html#fitting-the-model-ordinary-least-squares-ols","title":"Fitting the Model: Ordinary Least Squares (OLS)","text":"<p>The most common way to fit a linear regression model is with the ordinary least squares (OLS) method. The objective is to find parameters \\( \\boldsymbol{\\beta} \\) that minimise the following cost function:</p> \\[ J(\\boldsymbol{\\beta}) = \\frac{1}{2} \\sum_{i=1}^{M} \\left(h_{\\boldsymbol{\\beta}}(X_i) - y_i\\right)^2, \\] <p>where:</p> <ul> <li>\\( \\boldsymbol{\\beta} = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_N \\end{bmatrix} \\) \u2014 Vector of model parameters.</li> <li>\\( X_i = \\begin{bmatrix} 1 &amp; x_{i,1} &amp; x_{i,2} &amp; \\dots &amp; x_{i,N} \\end{bmatrix} \\) \u2014 Row vector of features for sample \\( i \\), including a 1 to account for the bias term \\( \\beta_0 \\).</li> <li>\\( h_{\\boldsymbol{\\beta}}(X_i) \\) \u2014 The hypothesis (model) function, defined as:</li> </ul> \\[ h_{\\boldsymbol{\\beta}}(X_i) = \\beta_0 + \\beta_1 x_{i,1} + \\dots + \\beta_N x_{i,N} = \\boldsymbol{\\beta}^\\top X_i^\\top. \\] <p>Thus, we are minimising the sum of squared differences between the predicted outputs \\( h_{\\boldsymbol{\\beta}}(X_i) \\) and the actual outputs \\( y_i \\) across all training samples.</p>"},{"location":"linear_regression.html#closed-form-solution","title":"Closed-Form Solution","text":"<p>To simplify notation and enable vectorised computation, we define:</p> <ul> <li>The design matrix:</li> </ul> \\[ X = \\begin{bmatrix} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_M \\end{bmatrix} \\quad \\text{(an \\( M \\times (N+1) \\) matrix)} \\] <ul> <li>The output vector:</li> </ul> \\[ \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_M \\end{bmatrix} \\] <p>OLS is one of the few machine learning methods with a closed-form analytic solution for the best-fit parameters. The solution is:</p> \\[ \\boldsymbol{\\beta} = (X^\\top X)^{-1} X^\\top \\mathbf{y} \\] <p>This gives the value of \\( \\boldsymbol{\\beta} \\) that minimises the cost function and fits the training data as closely as possible in the least-squares sense.</p>"},{"location":"linear_regression.html#example-from-r","title":"Example from <code>R</code>","text":"<p>The programming language <code>R</code> is widely used in statistics and data visualisation. It also provides a useful library of built-in datasets.</p> <p>To explore this, navigate to the folder <code>./codes/</code> in the repository. Activate the virtual environment we created earlier (see Requirements).</p> <p>Then, launch Jupyter Notebook:</p> <pre><code>jupyter notebook\n</code></pre> <p>Open the notebook named <code>linear_regression.ipynb</code>, which contains <code>R</code> code. Make sure you use the <code>R</code> kernel. </p> <p>We begin by importing <code>datasets</code> package:</p> <pre><code>library(datasets)\n</code></pre> <p>This package includes several classic datasets, one of which is <code>USJudgeRatings</code>. This dataset contains average ratings for 43 judges across several dimensions (e.g., integrity, diligence, writing quality). The 12th column is the rating for \u201cworthy of retention,\u201d which we\u2019ll try to predict using the first 11 features.</p> <p><pre><code>dataset = USJudgeRatings\nhead(dataset)\n</code></pre> This command prints the first six rows of the dataset:</p> US Judge Ratings CONTINTGDMNRDILGCFMGDECIPREPFAMIORALWRITPHYSRTEN AARONSON,L.H.5.77.97.77.37.17.47.17.17.17.08.37.8 ALEXANDER,J.M.6.88.98.88.57.88.18.08.07.87.98.58.7 ARMENTANO,A.J.7.28.17.87.87.57.67.57.57.37.47.97.8 BERDON,R.I.6.88.88.58.88.38.58.78.78.48.58.88.7 BRACKEN,J.J.7.36.44.36.56.06.25.75.75.15.35.54.8 BURNS,E.B.6.28.88.78.57.98.08.18.08.08.08.68.6 <p>To prepare the data and run linear regression:</p> <pre><code>x &lt;- as.matrix(dataset[, c(1:11)]) # First 11 dimensions are Features\ny &lt;- as.matrix(dataset[, 12]) # last dimeions is the Output (Retention rating)\nreg &lt;- lm(y ~ x)\n</code></pre> <p>This creates matrices <code>x</code> and <code>y</code> and uses the <code>lm</code> function to perform linear regression.</p> <p>To see the model summary:</p> <pre><code>summary(reg)\n</code></pre> <p>This produces several useful statistics:</p> <pre><code>Call:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max\n-0.22123 -0.06155 -0.01055  0.05045  0.26079\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -2.11943    0.51904  -4.083 0.000290 ***\nxCONT        0.01280    0.02586   0.495 0.624272\nxINTG        0.36484    0.12936   2.820 0.008291 **\nxDMNR        0.12540    0.08971   1.398 0.172102\nxDILG        0.06669    0.14303   0.466 0.644293\nxCFMG       -0.19453    0.14779  -1.316 0.197735\nxDECI        0.27829    0.13826   2.013 0.052883 .\nxPREP       -0.00196    0.24001  -0.008 0.993536\nxFAMI       -0.13579    0.26725  -0.508 0.614972\nxORAL        0.54782    0.27725   1.976 0.057121 .\nxWRIT       -0.06806    0.31485  -0.216 0.830269\nxPHYS        0.26881    0.06213   4.326 0.000146 ***\n---\nSignif. codes:  0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1\n\nResidual standard error: 0.1174 on 31 degrees of freedom\nMultiple R-squared:  0.9916,    Adjusted R-squared:  0.9886\nF-statistic: 332.9 on 11 and 31 DF,  p-value: &lt; 2.2e-16\n</code></pre> <p>I don't know about you, but I think this is mega. However, we\u2019ll take it a step further and write our own <code>Python</code> code \u2014 mostly from scratch \u2014 to calculate these statistics ourselves and better understand what they mean.</p>"},{"location":"linear_regression.html#example-from-python","title":"Example from Python","text":""},{"location":"requirements.html","title":"Requirements","text":"<p>First off, I would recommend using a Conda virual environment:</p> <pre><code>conda create --name ML_train\nconda activate ML_train\n</code></pre> <pre><code>conda install -c conda-forge python ipython jupyterlab \nconda install -c conda-forge scipy pandas numpy networkx scikit-learn\nconda install -c conda-forge nbconvert matplotlib seaborn plotly\nconda install -c conda-forge streamlit scikit-plot sqlite grip\npip3 install torch torchvision torchaudio\npip3 install mkdocs mkdocs-material\npip3 install mkdocs-pymdownx-material-extras\nconda install -c conda-forge r-base r-essentials r-pacman r-psychtools r-lars\n</code></pre> <p>We probably don't need all of these packages, but this is the environment I'm working with. You can also try:</p> <pre><code>    conda env create -f requirements_mac.yml\n</code></pre> <p>using the requirements file in the main directory. I have typically found that this only works on the same OS though.</p>"},{"location":"requirements.html#some-notes","title":"Some notes","text":"<ul> <li><code>scikit-learn</code> and <code>torch</code> are for machine learning.</li> <li><code>sqlite</code> is used to manipulate SQL databases using SQL in Python. (Very useful data structures for industry).</li> <li><code>mkdocs</code> is a great package for making code documentation websites (Like this one!).</li> <li><code>nbconvert</code> converts Jupyter notebooks to .py files. </li> <li><code>matplotlib</code>, <code>seaborn</code>, <code>plotly</code> is for plotting and data visualisation, <code>streamlit</code> is for dashboards.</li> <li><code>r-base</code>, <code>r-essentials</code>, <code>r-pacman</code> will let us use <code>R</code>. <code>r-pacman</code> is a package manager that will automatically download required packages from CRAN. For deployment, it might be better to download packages using <code>conda</code>. This will work fine for us though.</li> <li><code>r-psychtools</code> has the package <code>psych</code> which contains useful functions like <code>describe()</code>. On my Mac, I needed to have XCode installed first for this to work.</li> </ul>"}]}